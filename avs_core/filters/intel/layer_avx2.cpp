// AviSynth+.  Copyright 2026- AviSynth+ Project
// https://avs-plus.net
// http://avisynth.nl
// This program is free software; you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation; either version 2 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program; if not, write to the Free Software
// Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA, or visit
// http://www.gnu.org/copyleft/gpl.html .
//
// Linking Avisynth statically or dynamically with other modules is making a
// combined work based on Avisynth.  Thus, the terms and conditions of the GNU
// General Public License cover the whole combination.
//
// As a special exception, the copyright holders of Avisynth give you
// permission to link Avisynth with independent modules that communicate with
// Avisynth solely through the interfaces defined in avisynth.h, regardless of the license
// terms of these independent modules, and to copy and distribute the
// resulting combined work under terms of your choice, provided that
// every copy of the combined work is accompanied by a complete copy of
// the source code of Avisynth (the version of Avisynth used to produce the
// combined work), being distributed under the terms of the GNU General
// Public License plus this exception.  An independent module is a module
// which is not derived from or based on Avisynth, such as 3rd-party filters,
// import and export plugins, or graphical user interfaces.

#include "../layer.h"
#include "layer_avx2.h"

#if defined(_MSC_VER)
#include <intrin.h> // MSVC
#else 
#include <x86intrin.h> // GCC/MinGW/Clang/LLVM
#endif
#include <immintrin.h>

#include <cstdint>

#include <avs/minmax.h>
#include <avs/alignment.h>
#include "../core/internal.h"

#include "../convert/convert_planar.h"
#include <algorithm>

// Mostly RGB32 stuff, unaligned addresses, pixels grouped by 4

static AVS_FORCEINLINE __m128i mask_core_avx2(__m128i& src, __m128i& alpha, __m128i& not_alpha_mask, __m128i& zero, __m128i& matrix, __m128i& round_mask) {
  __m128i not_alpha = _mm_and_si128(src, not_alpha_mask);

  __m128i pixel0 = _mm_unpacklo_epi8(alpha, zero);
  __m128i pixel1 = _mm_unpackhi_epi8(alpha, zero);

  pixel0 = _mm_madd_epi16(pixel0, matrix);
  pixel1 = _mm_madd_epi16(pixel1, matrix);

  __m128i tmp = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(pixel0), _mm_castsi128_ps(pixel1), _MM_SHUFFLE(3, 1, 3, 1)));
  __m128i tmp2 = _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(pixel0), _mm_castsi128_ps(pixel1), _MM_SHUFFLE(2, 0, 2, 0)));

  tmp = _mm_add_epi32(tmp, tmp2);
  tmp = _mm_add_epi32(tmp, round_mask);
  tmp = _mm_srli_epi32(tmp, 15);
  __m128i result_alpha = _mm_slli_epi32(tmp, 24);

  return _mm_or_si128(result_alpha, not_alpha);
}

// called for RGB32
void mask_avx2(BYTE* srcp, const BYTE* alphap, int src_pitch, int alpha_pitch, size_t width, size_t height) {
  __m128i matrix = _mm_set_epi16(0, cyr, cyg, cyb, 0, cyr, cyg, cyb);
  __m128i zero = _mm_setzero_si128();
  __m128i round_mask = _mm_set1_epi32(16384);
  __m128i not_alpha_mask = _mm_set1_epi32(0x00FFFFFF);

  size_t width_bytes = width * 4;
  size_t width_mod16 = width_bytes / 16 * 16;

  for (size_t y = 0; y < height; ++y) {
    for (size_t x = 0; x < width_mod16; x += 16) {
      __m128i src = _mm_load_si128(reinterpret_cast<const __m128i*>(srcp + x));
      __m128i alpha = _mm_load_si128(reinterpret_cast<const __m128i*>(alphap + x));
      __m128i result = mask_core_avx2(src, alpha, not_alpha_mask, zero, matrix, round_mask);

      _mm_store_si128(reinterpret_cast<__m128i*>(srcp + x), result);
    }

    if (width_mod16 < width_bytes) {
      __m128i src = _mm_loadu_si128(reinterpret_cast<const __m128i*>(srcp + width_bytes - 16));
      __m128i alpha = _mm_loadu_si128(reinterpret_cast<const __m128i*>(alphap + width_bytes - 16));
      __m128i result = mask_core_avx2(src, alpha, not_alpha_mask, zero, matrix, round_mask);

      _mm_storeu_si128(reinterpret_cast<__m128i*>(srcp + width_bytes - 16), result);
    }

    srcp += src_pitch;
    alphap += alpha_pitch;
  }
}

void colorkeymask_avx2(BYTE* pf, int pitch, int color, int height, int width, int tolB, int tolG, int tolR) {
  unsigned int t = 0xFF000000 | (tolR << 16) | (tolG << 8) | tolB;
  __m128i tolerance = _mm_set1_epi32(t);
  __m128i colorv = _mm_set1_epi32(color);
  __m128i zero = _mm_setzero_si128();

  BYTE* endp = pf + pitch * height;

  while (pf < endp)
  {
    __m128i src = _mm_load_si128(reinterpret_cast<const __m128i*>(pf));
    __m128i gt = _mm_subs_epu8(colorv, src);
    __m128i lt = _mm_subs_epu8(src, colorv);
    __m128i absdiff = _mm_or_si128(gt, lt); //abs(color - src)

    __m128i not_passed = _mm_subs_epu8(absdiff, tolerance);
    __m128i passed = _mm_cmpeq_epi32(not_passed, zero);
    passed = _mm_slli_epi32(passed, 24);
    __m128i result = _mm_andnot_si128(passed, src);

    _mm_store_si128(reinterpret_cast<__m128i*>(pf), result);

    pf += 16;
  }
}

// by 4 bytes, when rgba mask can be separate FF bytes, for plane FF FF FF FF
// to simple, even C is identical speed
void invert_frame_inplace_avx2(BYTE* frame, int pitch, int width, int height, int mask) {
  __m256i maskv = _mm256_set1_epi32(mask);

  BYTE* endp = frame + pitch * height;
  // geee, no y loop
  while (frame < endp) {
    __m256i src = _mm256_load_si256(reinterpret_cast<const __m256i*>(frame));
    __m256i inv = _mm256_xor_si256(src, maskv);
    _mm256_store_si256(reinterpret_cast<__m256i*>(frame), inv);
    frame += 32;
  }
}

// to simple, even C is identical speed
void invert_frame_uint16_inplace_avx2(BYTE* frame, int pitch, int width, int height, uint64_t mask64) {
  __m256i maskv = _mm256_set_epi32(
    (uint32_t)(mask64 >> 32), (uint32_t)mask64, (uint32_t)(mask64 >> 32), (uint32_t)mask64,
    (uint32_t)(mask64 >> 32), (uint32_t)mask64, (uint32_t)(mask64 >> 32), (uint32_t)mask64);

  BYTE* endp = frame + pitch * height;
  // geee, no y loop
  while (frame < endp) {
    __m256i src = _mm256_load_si256(reinterpret_cast<const __m256i*>(frame));
    __m256i inv = _mm256_xor_si256(src, maskv);
    _mm256_store_si256(reinterpret_cast<__m256i*>(frame), inv);
    frame += 32;
  }
}

// called for uint8_t, uint16_t and float planar, chroma planes are inverted differently than luma plane
// R G B are treated the same way as luma.
// We assume full-range.
// 3.7.6 minor change: chroma: uint8_t, uint16_t: pivot around half and not xor FF/FFFF for chroma
// Note: this filter is so simple that it is optimized from C to same speed as SIMD in release
// Also, it is memory-bound AVX2 is not quicker than SSE2.
// lessthan16bits helps optimizing the exact 16 bit case.
// We use this very same C source for AVX2, where it is optimized even with 2x256 bit paths
template<typename pixel_t, bool lessthan16bits, bool chroma>
void invert_plane_c_avx2(uint8_t* dstp, const uint8_t* srcp, int src_pitch, int dst_pitch, int width, int height, int bits_per_pixel) {
  if constexpr (std::is_same_v<pixel_t, float>) {
    if constexpr (chroma) {
      // For chroma planes, invert around 0.0 -> negate
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<float*>(dstp)[x] = -reinterpret_cast<const float*>(srcp)[x];
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
    else {
      // For luma plane, invert around 1.0 -> 1.0 - value
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<float*>(dstp)[x] = 1.0f - reinterpret_cast<const float*>(srcp)[x];
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
    return;
  }
  // 8 bit
  if constexpr (std::is_same_v<pixel_t, uint8_t>) {
    constexpr int max_pixel_value = 255;
    if constexpr (chroma) {
      constexpr int half = 128;
      // For chroma planes, invert around 128 -> negate
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<uint8_t*>(dstp)[x] = std::min(2 * half - reinterpret_cast<const uint8_t*>(srcp)[x], max_pixel_value);
          // chroma invert: -(srcp[x] - half) + half
          // = 2*half - srcp[x] = (1 << bits_per_pixel) - srcp[x]
          // Watch for src==0, must top at max_pixel_value
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
    else {
      // For luma plane, 255-x which is xor 0xff
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<uint8_t*>(dstp)[x] = max_pixel_value - reinterpret_cast<const uint8_t*>(srcp)[x];
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
    return;
  }
  // 10-16 bit uint16_t, luma: max_pixel_value - x which is xor with max_pixel_value, chroma: half - x
  if constexpr (std::is_same_v<pixel_t, uint16_t>) {
    if constexpr (!lessthan16bits)
      bits_per_pixel = 16; // quasi constexpr for optimization
    const int max_pixel_value = (1 << bits_per_pixel) - 1;
    if constexpr (chroma) {
      const int half = 1 << (bits_per_pixel - 1);
      // For chroma planes, invert around mid-point (2^(bits-1))
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<uint16_t*>(dstp)[x] = std::min(2 * half - reinterpret_cast<const uint16_t*>(srcp)[x], max_pixel_value);
          // chroma invert: -(srcp[x] - half) + half
          // = 2*half - srcp[x] = (1 << bits_per_pixel) - srcp[x]
          // Watch for src==0, must top at max_pixel_value
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
    else {
      // For luma plane, max_pixel_value - x
      for (int y = 0; y < height; ++y) {
        for (int x = 0; x < width; ++x) {
          reinterpret_cast<uint16_t*>(dstp)[x] = max_pixel_value - reinterpret_cast<const uint16_t*>(srcp)[x];
        }
        srcp += src_pitch;
        dstp += dst_pitch;
      }
    }
  }
}

// Instantiate all AVX2 combinations
template void invert_plane_c_avx2<uint8_t, true /*n/a*/, false>(uint8_t*, const uint8_t*, int, int, int, int, int);
template void invert_plane_c_avx2<uint8_t, true /*n/a*/, true>(uint8_t*, const uint8_t*, int, int, int, int, int);

template void invert_plane_c_avx2<uint16_t, true, false>(uint8_t*, const uint8_t*, int, int, int, int, int);
template void invert_plane_c_avx2<uint16_t, true, true>(uint8_t*, const uint8_t*, int, int, int, int, int);

template void invert_plane_c_avx2<uint16_t, false, false>(uint8_t*, const uint8_t*, int, int, int, int, int);
template void invert_plane_c_avx2<uint16_t, false, true>(uint8_t*, const uint8_t*, int, int, int, int, int);

template void invert_plane_c_avx2<float, false /*n/a*/, false>(uint8_t*, const uint8_t*, int, int, int, int, int);
template void invert_plane_c_avx2<float, false /*n/a*/, true>(uint8_t*, const uint8_t*, int, int, int, int, int);


/*******************************
 *******   Layer Filter   ******
 *******************************/

// "fast" blend is simple averaging
template<typename pixel_t>
void layer_genericplane_fast_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  AVS_UNUSED(level);
  int width_bytes = width * sizeof(pixel_t);
  int width_mod32 = width_bytes / 32 * 32;

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < width_mod32; x += 32) {
      __m256i src = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(dstp + x));
      __m256i ovr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ovrp + x));
      if constexpr (sizeof(pixel_t) == 1)
        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dstp + x), _mm256_avg_epu8(src, ovr));
      else
        _mm256_storeu_si256(reinterpret_cast<__m256i*>(dstp + x), _mm256_avg_epu16(src, ovr));
    }

    for (int x = width_mod32 / sizeof(pixel_t); x < width; ++x) {
      reinterpret_cast<pixel_t*>(dstp)[x] = (reinterpret_cast<pixel_t*>(dstp)[x] + reinterpret_cast<const pixel_t*>(ovrp)[x] + 1) / 2;
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// instantiate
template void layer_genericplane_fast_avx2<uint8_t>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);
template void layer_genericplane_fast_avx2<uint16_t>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);

/* RGB32 */

//src format: xx xx xx xx | xx xx xx xx | a1 xx xx xx | a0 xx xx xx
//level_vector and one should be vectors of 32bit packed integers
static AVS_FORCEINLINE __m128i calculate_monochrome_alpha_avx2(const __m128i& src, const __m128i& level_vector, const __m128i& one) {
  __m128i alpha = _mm_srli_epi32(src, 24);
  alpha = _mm_mullo_epi16(alpha, level_vector);
  alpha = _mm_add_epi32(alpha, one);
  alpha = _mm_srli_epi32(alpha, 8);
  alpha = _mm_shufflelo_epi16(alpha, _MM_SHUFFLE(2, 2, 0, 0));
  return _mm_shuffle_epi32(alpha, _MM_SHUFFLE(1, 1, 0, 0));
}

static AVS_FORCEINLINE __m128i calculate_luma_avx2(const __m128i& src, const __m128i& rgb_coeffs, const __m128i& zero) {
  AVS_UNUSED(zero);
  __m128i temp = _mm_madd_epi16(src, rgb_coeffs);
  __m128i low = _mm_shuffle_epi32(temp, _MM_SHUFFLE(3, 3, 1, 1));
  temp = _mm_add_epi32(low, temp);
  temp = _mm_srli_epi32(temp, 15);
  __m128i result = _mm_shufflelo_epi16(temp, _MM_SHUFFLE(0, 0, 0, 0));
  return _mm_shufflehi_epi16(result, _MM_SHUFFLE(0, 0, 0, 0));
}

// must be unaligned load/store
template<bool use_chroma>
void layer_rgb32_mul_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  int mod2_width = width / 2 * 2;

  __m128i zero = _mm_setzero_si128();
  __m128i level_vector = _mm_set1_epi32(level);
  __m128i one = _mm_set1_epi32(1);
  __m128i rgb_coeffs = _mm_set_epi16(0, cyr, cyg, cyb, 0, cyr, cyg, cyb);

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < mod2_width; x += 2) {
      __m128i src = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(dstp + x * 4));
      __m128i ovr = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ovrp + x * 4));

      __m128i alpha = calculate_monochrome_alpha_avx2(ovr, level_vector, one);

      src = _mm_unpacklo_epi8(src, zero);
      ovr = _mm_unpacklo_epi8(ovr, zero);

      __m128i luma;
      if (use_chroma) {
        luma = ovr;
      }
      else {
        luma = calculate_luma_avx2(ovr, rgb_coeffs, zero);
      }

      __m128i dst = _mm_mullo_epi16(luma, src);
      dst = _mm_srli_epi16(dst, 8);
      dst = _mm_subs_epi16(dst, src);
      dst = _mm_mullo_epi16(dst, alpha);
      dst = _mm_srli_epi16(dst, 8);
      dst = _mm_add_epi8(src, dst);

      dst = _mm_packus_epi16(dst, zero);

      _mm_storel_epi64(reinterpret_cast<__m128i*>(dstp + x * 4), dst);
    }

    if (width != mod2_width) {
      int x = mod2_width;
      int alpha = (ovrp[x * 4 + 3] * level + 1) >> 8;

      if (use_chroma) {
        dstp[x * 4] = dstp[x * 4] + (((((ovrp[x * 4] * dstp[x * 4]) >> 8) - dstp[x * 4]) * alpha) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((((ovrp[x * 4 + 1] * dstp[x * 4 + 1]) >> 8) - dstp[x * 4 + 1]) * alpha) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((((ovrp[x * 4 + 2] * dstp[x * 4 + 2]) >> 8) - dstp[x * 4 + 2]) * alpha) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((((ovrp[x * 4 + 3] * dstp[x * 4 + 3]) >> 8) - dstp[x * 4 + 3]) * alpha) >> 8);
      }
      else {
        int luma = (cyb * ovrp[x * 4] + cyg * ovrp[x * 4 + 1] + cyr * ovrp[x * 4 + 2]) >> 15;

        dstp[x * 4] = dstp[x * 4] + (((((luma * dstp[x * 4]) >> 8) - dstp[x * 4]) * alpha) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((((luma * dstp[x * 4 + 1]) >> 8) - dstp[x * 4 + 1]) * alpha) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((((luma * dstp[x * 4 + 2]) >> 8) - dstp[x * 4 + 2]) * alpha) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((((luma * dstp[x * 4 + 3]) >> 8) - dstp[x * 4 + 3]) * alpha) >> 8);
      }
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// instantiate
template void layer_rgb32_mul_avx2<false>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);
template void layer_rgb32_mul_avx2<true>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);


// must be unaligned load/store
template<bool use_chroma>
void layer_rgb32_add_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  int mod2_width = width / 2 * 2;

  __m128i zero = _mm_setzero_si128();
  __m128i level_vector = _mm_set1_epi32(level);
  __m128i one = _mm_set1_epi32(1);
  __m128i rgb_coeffs = _mm_set_epi16(0, cyr, cyg, cyb, 0, cyr, cyg, cyb);

  constexpr int rounder = 128;
  const __m128i rounder_simd = _mm_set1_epi16(rounder);

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < mod2_width; x += 2) {
      __m128i src = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(dstp + x * 4));
      __m128i ovr = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ovrp + x * 4));

      __m128i alpha = calculate_monochrome_alpha_avx2(ovr, level_vector, one);

      src = _mm_unpacklo_epi8(src, zero);
      ovr = _mm_unpacklo_epi8(ovr, zero);

      __m128i luma;
      if (use_chroma) {
        luma = ovr;
      }
      else {
        luma = calculate_luma_avx2(ovr, rgb_coeffs, zero);
      }

      __m128i dst = _mm_subs_epi16(luma, src);
      dst = _mm_mullo_epi16(dst, alpha);
      dst = _mm_add_epi16(dst, rounder_simd);
      dst = _mm_srli_epi16(dst, 8);
      dst = _mm_add_epi8(src, dst);

      dst = _mm_packus_epi16(dst, zero);

      _mm_storel_epi64(reinterpret_cast<__m128i*>(dstp + x * 4), dst);
    }

    if (width != mod2_width) {
      int x = mod2_width;
      int alpha = (ovrp[x * 4 + 3] * level + 1) >> 8;

      if (use_chroma) {
        dstp[x * 4] = dstp[x * 4] + (((ovrp[x * 4] - dstp[x * 4]) * alpha + rounder) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((ovrp[x * 4 + 1] - dstp[x * 4 + 1]) * alpha + rounder) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((ovrp[x * 4 + 2] - dstp[x * 4 + 2]) * alpha + rounder) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((ovrp[x * 4 + 3] - dstp[x * 4 + 3]) * alpha + rounder) >> 8);
      }
      else {
        int luma = (cyb * ovrp[x * 4] + cyg * ovrp[x * 4 + 1] + cyr * ovrp[x * 4 + 2]) >> 15;

        dstp[x * 4] = dstp[x * 4] + (((luma - dstp[x * 4]) * alpha + rounder) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((luma - dstp[x * 4 + 1]) * alpha + rounder) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((luma - dstp[x * 4 + 2]) * alpha + rounder) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((luma - dstp[x * 4 + 3]) * alpha + rounder) >> 8);
      }
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// instantiate
template void layer_rgb32_add_avx2<false>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);
template void layer_rgb32_add_avx2<true>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);

// unlike sse2 alignment is not required. avx2 has no such big penalty
void layer_yuy2_or_rgb32_fast_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  AVS_UNUSED(level);
  int width_bytes = width * 2;
  int width_mod32 = width_bytes / 32 * 32;

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < width_mod32; x += 32) {
      __m256i src = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(dstp + x));
      __m256i ovr = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(ovrp + x));

      _mm256_storeu_si256(reinterpret_cast<__m256i*>(dstp + x), _mm256_avg_epu8(src, ovr));
    }

    for (int x = width_mod32; x < width_bytes; ++x) {
      dstp[x] = (dstp[x] + ovrp[x] + 1) / 2;
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// aligned ptr not required
void layer_rgb32_fast_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  layer_yuy2_or_rgb32_fast_avx2(dstp, ovrp, dst_pitch, overlay_pitch, width * 2, height, level);
}

// unaligned addresses
template<bool use_chroma>
void layer_rgb32_subtract_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level) {
  int mod2_width = width / 2 * 2;

  __m128i zero = _mm_setzero_si128();
  __m128i level_vector = _mm_set1_epi32(level);
  __m128i one = _mm_set1_epi32(1);
  __m128i rgb_coeffs = _mm_set_epi16(0, cyr, cyg, cyb, 0, cyr, cyg, cyb);
  __m128i ff = _mm_set1_epi16(0x00FF);

  constexpr int rounder = 128;
  const __m128i rounder_simd = _mm_set1_epi16(rounder);

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < mod2_width; x += 2) {
      __m128i src = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(dstp + x * 4));
      __m128i ovr = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ovrp + x * 4));

      __m128i alpha = calculate_monochrome_alpha_avx2(ovr, level_vector, one);

      src = _mm_unpacklo_epi8(src, zero);
      ovr = _mm_unpacklo_epi8(ovr, zero);

      __m128i luma;
      if (use_chroma) {
        luma = _mm_subs_epi16(ff, ovr);
      }
      else {
        luma = calculate_luma_avx2(_mm_andnot_si128(ovr, ff), rgb_coeffs, zero);
      }

      __m128i dst = _mm_subs_epi16(luma, src);
      dst = _mm_mullo_epi16(dst, alpha);
      dst = _mm_add_epi16(dst, rounder_simd);
      dst = _mm_srli_epi16(dst, 8);
      dst = _mm_add_epi8(src, dst);

      dst = _mm_packus_epi16(dst, zero);

      _mm_storel_epi64(reinterpret_cast<__m128i*>(dstp + x * 4), dst);
    }

    if (width != mod2_width) {
      int x = mod2_width;
      int alpha = (ovrp[x * 4 + 3] * level + 1) >> 8;

      if (use_chroma) {
        dstp[x * 4] = dstp[x * 4] + (((255 - ovrp[x * 4] - dstp[x * 4]) * alpha + rounder) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((255 - ovrp[x * 4 + 1] - dstp[x * 4 + 1]) * alpha + rounder) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((255 - ovrp[x * 4 + 2] - dstp[x * 4 + 2]) * alpha + rounder) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((255 - ovrp[x * 4 + 3] - dstp[x * 4 + 3]) * alpha + rounder) >> 8);
      }
      else {
        int luma = (cyb * (255 - ovrp[x * 4]) + cyg * (255 - ovrp[x * 4 + 1]) + cyr * (255 - ovrp[x * 4 + 2])) >> 15;

        dstp[x * 4] = dstp[x * 4] + (((luma - dstp[x * 4]) * alpha + rounder) >> 8);
        dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((luma - dstp[x * 4 + 1]) * alpha + rounder) >> 8);
        dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((luma - dstp[x * 4 + 2]) * alpha + rounder) >> 8);
        dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((luma - dstp[x * 4 + 3]) * alpha + rounder) >> 8);
      }
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// instantiate
template void layer_rgb32_subtract_avx2<false>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);
template void layer_rgb32_subtract_avx2<true>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level);

// unaligned adresses
template<int mode>
void layer_rgb32_lighten_darken_avx2(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level, int thresh) {
  int mod2_width = width / 2 * 2;

  __m128i zero = _mm_setzero_si128();
  __m128i level_vector = _mm_set1_epi32(level);
  __m128i one = _mm_set1_epi32(1);
  __m128i rgb_coeffs = _mm_set_epi16(0, cyr, cyg, cyb, 0, cyr, cyg, cyb);
  __m128i threshold = _mm_set1_epi16(thresh);

  constexpr int rounder = 128;
  const __m128i rounder_simd = _mm_set1_epi16(rounder);

  for (int y = 0; y < height; ++y) {
    for (int x = 0; x < mod2_width; x += 2) {
      __m128i src = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(dstp + x * 4));
      __m128i ovr = _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ovrp + x * 4));

      __m128i alpha = calculate_monochrome_alpha_avx2(ovr, level_vector, one);

      src = _mm_unpacklo_epi8(src, zero);
      ovr = _mm_unpacklo_epi8(ovr, zero);

      __m128i luma_ovr = calculate_luma_avx2(ovr, rgb_coeffs, zero);
      __m128i luma_src = calculate_luma_avx2(src, rgb_coeffs, zero);

      __m128i mask;
      if constexpr (mode == LIGHTEN) {
        __m128i tmp = _mm_add_epi16(luma_src, threshold);
        mask = _mm_cmpgt_epi16(luma_ovr, tmp);
      }
      else {
        __m128i tmp = _mm_sub_epi16(luma_src, threshold);
        mask = _mm_cmpgt_epi16(tmp, luma_ovr);
      }

      alpha = _mm_and_si128(alpha, mask);

      __m128i dst = _mm_subs_epi16(ovr, src);
      dst = _mm_mullo_epi16(dst, alpha);
      dst = _mm_add_epi16(dst, rounder_simd);
      dst = _mm_srli_epi16(dst, 8);
      dst = _mm_add_epi8(src, dst);

      dst = _mm_packus_epi16(dst, zero);

      _mm_storel_epi64(reinterpret_cast<__m128i*>(dstp + x * 4), dst);
    }

    if (width != mod2_width) {
      int x = mod2_width;
      int alpha = (ovrp[x * 4 + 3] * level + 1) >> 8;
      int luma_ovr = (cyb * ovrp[x * 4] + cyg * ovrp[x * 4 + 1] + cyr * ovrp[x * 4 + 2]) >> 15;
      int luma_src = (cyb * dstp[x * 4] + cyg * dstp[x * 4 + 1] + cyr * dstp[x * 4 + 2]) >> 15;

      if constexpr (mode == LIGHTEN)
        alpha = luma_ovr > luma_src + thresh ? alpha : 0;
      else // DARKEN
        alpha = luma_ovr < luma_src - thresh ? alpha : 0;

      dstp[x * 4] = dstp[x * 4] + (((ovrp[x * 4] - dstp[x * 4]) * alpha + rounder) >> 8);
      dstp[x * 4 + 1] = dstp[x * 4 + 1] + (((ovrp[x * 4 + 1] - dstp[x * 4 + 1]) * alpha + rounder) >> 8);
      dstp[x * 4 + 2] = dstp[x * 4 + 2] + (((ovrp[x * 4 + 2] - dstp[x * 4 + 2]) * alpha + rounder) >> 8);
      dstp[x * 4 + 3] = dstp[x * 4 + 3] + (((ovrp[x * 4 + 3] - dstp[x * 4 + 3]) * alpha + rounder) >> 8);
    }

    dstp += dst_pitch;
    ovrp += overlay_pitch;
  }
}

// instantiate
template void layer_rgb32_lighten_darken_avx2<LIGHTEN>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level, int thresh);
template void layer_rgb32_lighten_darken_avx2<DARKEN>(BYTE* dstp, const BYTE* ovrp, int dst_pitch, int overlay_pitch, int width, int height, int level, int thresh);

